# Revo Library - Multiple Models Configuration Example
# This file shows how to configure multiple LLM models using environment variables
# 
# IMPORTANT: This approach uses a workaround since pydantic-settings doesn't
# natively support nested dictionaries with dynamic keys through environment variables.
# 
# For production use, we recommend using YAML or JSON configuration files instead.

# =============================================================================
# REQUIRED REVO API CREDENTIALS
# =============================================================================

REVO_CLIENT_ID=your_revo_client_id_here
REVO_CLIENT_SECRET=your_revo_client_secret_here

# =============================================================================
# REVO API SETTINGS
# =============================================================================

REVO_TOKEN_URL=https://your-site.com/revo/oauth/token
REVO_BASE_URL=https://your-site.com/revo/llm-api
REVO_TOKEN_BUFFER_MINUTES=5
REVO_MAX_RETRIES=3
REVO_REQUEST_TIMEOUT=30

# =============================================================================
# MULTIPLE LLM MODELS CONFIGURATION
# =============================================================================

# This approach uses environment variables to configure multiple models
# by setting individual model configurations with a naming pattern.
# 
# Pattern: LLM_MODELS_MODEL_NAME_SETTING_NAME=value
# 
# Example for "gpt-3.5-turbo" model:
LLM_MODELS_GPT_3_5_TURBO_MODEL=gpt-3.5-turbo
LLM_MODELS_GPT_3_5_TURBO_TEMPERATURE=0.1
LLM_MODELS_GPT_3_5_TURBO_MAX_TOKENS=1000
LLM_MODELS_GPT_3_5_TURBO_TOP_P=1.0
LLM_MODELS_GPT_3_5_TURBO_FREQUENCY_PENALTY=0.0
LLM_MODELS_GPT_3_5_TURBO_PRESENCE_PENALTY=0.0
LLM_MODELS_GPT_3_5_TURBO_DESCRIPTION=Fast and cost-effective model for general tasks

# Example for "gpt-4" model:
LLM_MODELS_GPT_4_MODEL=gpt-4
LLM_MODELS_GPT_4_TEMPERATURE=0.1
LLM_MODELS_GPT_4_MAX_TOKENS=2000
LLM_MODELS_GPT_4_TOP_P=1.0
LLM_MODELS_GPT_4_FREQUENCY_PENALTY=0.0
LLM_MODELS_GPT_4_PRESENCE_PENALTY=0.0
LLM_MODELS_GPT_4_DESCRIPTION=Most capable model for complex tasks

# Example for "gpt-4-turbo" model:
LLM_MODELS_GPT_4_TURBO_MODEL=gpt-4-turbo
LLM_MODELS_GPT_4_TURBO_TEMPERATURE=0.1
LLM_MODELS_GPT_4_TURBO_MAX_TOKENS=4000
LLM_MODELS_GPT_4_TURBO_TOP_P=1.0
LLM_MODELS_GPT_4_TURBO_FREQUENCY_PENALTY=0.0
LLM_MODELS_GPT_4_TURBO_PRESENCE_PENALTY=0.0
LLM_MODELS_GPT_4_TURBO_DESCRIPTION=Latest GPT-4 model with extended context

# Example for "creative" model (custom name):
LLM_MODELS_CREATIVE_MODEL=gpt-4
LLM_MODELS_CREATIVE_TEMPERATURE=0.8
LLM_MODELS_CREATIVE_MAX_TOKENS=2000
LLM_MODELS_CREATIVE_TOP_P=0.9
LLM_MODELS_CREATIVE_FREQUENCY_PENALTY=0.3
LLM_MODELS_CREATIVE_PRESENCE_PENALTY=0.3
LLM_MODELS_CREATIVE_DESCRIPTION=Creative model for brainstorming and ideation

# Example for "analytical" model (custom name):
LLM_MODELS_ANALYTICAL_MODEL=gpt-4
LLM_MODELS_ANALYTICAL_TEMPERATURE=0.0
LLM_MODELS_ANALYTICAL_MAX_TOKENS=3000
LLM_MODELS_ANALYTICAL_TOP_P=1.0
LLM_MODELS_ANALYTICAL_FREQUENCY_PENALTY=0.0
LLM_MODELS_ANALYTICAL_PRESENCE_PENALTY=0.0
LLM_MODELS_ANALYTICAL_DESCRIPTION=Analytical model for data analysis and reasoning

# Example for "claude-3" model:
LLM_MODELS_CLAUDE_3_MODEL=claude-3-sonnet
LLM_MODELS_CLAUDE_3_TEMPERATURE=0.1
LLM_MODELS_CLAUDE_3_MAX_TOKENS=2000
LLM_MODELS_CLAUDE_3_TOP_P=1.0
LLM_MODELS_CLAUDE_3_FREQUENCY_PENALTY=0.0
LLM_MODELS_CLAUDE_3_PRESENCE_PENALTY=0.0
LLM_MODELS_CLAUDE_3_DESCRIPTION=Anthropic's Claude 3 model

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s
LOG_FILE=/tmp/revo.log
LOG_MAX_SIZE=10485760
LOG_BACKUP_COUNT=5

# =============================================================================
# TOKEN MANAGEMENT CONFIGURATION
# =============================================================================

TOKEN_REFRESH_INTERVAL_MINUTES=45
TOKEN_MAX_FAILURES_BEFORE_FALLBACK=1
TOKEN_ENABLE_PERIODIC_REFRESH=true
TOKEN_ENABLE_FALLBACK=true

# =============================================================================
# GLOBAL SETTINGS
# =============================================================================

DEBUG=false

# =============================================================================
# USAGE NOTES
# =============================================================================

# To use this configuration:
# 1. Copy this file to .env
# 2. Update the values as needed
# 3. In your Python code, use:
#
#    from revo import RevoMainConfig
#    config = RevoMainConfig()
#    
#    # List available models
#    from revo import list_available_extractors
#    extractors = list_available_extractors(config)
#    print("Available models:", list(extractors.keys()))
#    
#    # Create extractor for specific model
#    from revo import get_langchain_extractor
#    extractor = get_langchain_extractor("gpt-4", config)
#
# NOTE: This approach requires the LLMModelsConfig to be updated to support
# this environment variable pattern. Currently, the library supports this
# through YAML/JSON files or programmatic configuration.
#
# For production use, we recommend using the YAML configuration file approach
# as shown in config_multiple_models.yaml.example
